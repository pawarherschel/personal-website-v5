#import "../../../public/utils.typ": blog-post, caution, todo

#show: blog-post.with(
  "The future I was promised cannot exist",
  draft: true,
  tilslut: (),
)

LLMs are net positive for programming, and net negative for humanity.

#caution(title: "Disclaimer")[
  the only time llms were involved in the process of writing was during the proofreading phase, just before I send to human proofreaders. the only words written by llms are grammatical corrections. kagi proofread works a shitload better than ltex-ls-plus, and let's not even talk about fucking grammarly or some other proprietary shit which is worse than ltex-ls
]

programming is in a unique position.

programming, now, is only done digitally, there's no physical aspect to it . in the years past, the act of programming involved  #todo[wikipedia link for "punch cards"], then, afaiu with #todo[wikipedia link for ed], the scroll back part of the terminal was printed in the physical sense. on a paper, in the real world, where you can touch the "programming".

An external observer might think that writing the code IS programming. However, programming IS NOT the act of typing out code.

in the past decade, the biggest (not the best) innovation in the software world are llms. there's very few things which can even come close to it. so why is it that the general sentiment towards them is negative. in fact it's so negative that the sentiment has been slowly spreading towards other things like ai in general, the fucking word algorithm, and data centers???

llms are a net positive for developers, not entirely positive mind you, but def net positive. you need experience to know exactly what you want to prompt to get the best out of ai, and I'm def not there, nor do I care to be there.

llms are a tool, and just like all tools, you need to learn when to use it, and when not to not .

I practically vibe coded via github copilot before it was a thing.

I wrote comments in a python script, and let the llm write the functions for me. For the core of my program, I didn't trust it, and copied code from stack overflow. my main "contribution" was guiding the llm into writing the functions.

in the main function I wrote some trash code to tie together functions and even then more than half of it was written by copilot. and this was in the days of chatgpt 3.

the script never left my ssd. it didn't even get backed up to my laptop and phone, I didn't have either of them back then.

this is how I understand llms. I think llms, for programming, come under the category of a more advanced lsp + auto complete. It "understands" the code like #todo[[tree sitter]], and completes the text like the primitive algorithm in... let's say.... #todo[Kate]. It sees partially written text, scans the document for words which start with same letters, and then suggests you the completed word. And when I say "understands" I don't mean that in the sense that it knows your requirements, I mean that in the sense that it understands the grammar. it knows what functions look like, what variable declaration looks like. another thing, while Kate uses the words in the document as it's database of words, llms have an internal database which has the most common ways to solve the problems. similar to Kate, it can suggest invalid statements, you can't tab your way to completion, you need to check if it's valid and is doing what you expect.

it's not inventing new things.

#todo[Find the video]

in a recent video [[tsoding]] proposes that people encode their knowledge as code, but only a minority of the people read the code. llms on the other hand were trained on the code and thus the knowledge is stored in them. they don't invent new things, they're just retrieving the knowledge that people encoded in their code but no one bothered to read.

I think of it as an advanced form of copy pasting

and thus, i also think that the code generated by the llm, is the responsibility of the human.

I think it's fine to use llms for things which will never be released under a license, or if it can be scruitinized under a patent, which really limits when you can use llms and the CEOs hate that.

however once it comes to licensing the code written by llms, its impossible. some of the code is MIT, which I understand to means you can use it however, where ever, doesn't matter. but, inadvertantly, some of the code will be gpl 3,which is a viral copyleft license which requires you to release the code under the same, or a compatible license.

does that mean all llm generated code should be under the strictest copy left license? no.

see, I don't trust these companies to only train the llms with publically available code. I expect that Microsoft allowed (at least) openai to train on private repositories.

so avoid openai models and you should be fine right? nope

I don't have any proof for it, or any idea how it could happen. but I have a hunch that the models are also trained on proprietary code and data.

and if I were the owner or cto of a company, i would just avoid getting into legal trouble with it.

maybe some of the proofreaders will know about some court case or whatever, I know meta is under one for training on non public domain books. tho, what else do you expect from Facebook

so what can they be used for? I'm not sure to be quite honest.

until there's a open source llm, and not just open weight, which has a public dataset, which only contains public domain data, maybe MIT, maybe Apache licensed data, and all the code used to train the llms, it's just too risky to use in production.

imo, until we can reproduce the same blob on our own, using community shared hardware, its not open source

but those llms will be trash until the ability to extract knowledge from the limited dataset improves.

now then, let's talk about the potential future which could have an open source llm, who's weights I got from, idfk, some fucking research group in china.
